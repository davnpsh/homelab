services:
  tailscale:
    # Ports used: 53, 80, 81, 443
    image: tailscale/tailscale:latest
    hostname: odin
    restart: unless-stopped
    labels:
      glance.name: Tailscale
      glance.icon: sh:tailscale-light
      glance.description: VPN
      glance.hide: false
    environment:
      - TS_AUTHKEY=${TS_AUTHKEY}
      # ---
      # According to documentation in https://tailscale.com/kb/1282/docker
      # --advertise-tags is an argument you can pass in to the Tailscale CLI in
      # a `tailscale up` command using TS_EXTRA_ARGS, but using this variable
      # with this argument will make the container exit with error, because, in reality,
      # you advertise tags in a `tailscale login` command, as seen in
      # https://tailscale.com/kb/1080/cli#login
      # ---
      #- TS_EXTRA_ARGS=--advertise-tags=tag:container
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=false
    volumes:
      - ./tailscale/state:/var/lib/tailscale
    devices:
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - net_admin
    healthcheck:
      test: ["CMD-SHELL", "tailscale status --self"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  proxy:
    # Ports used: see Tailscale
    image: "jc21/nginx-proxy-manager:latest"
    restart: unless-stopped
    labels:
      glance.name: Proxy
      glance.icon: sh:nginx-proxy-manager-light
      glance.url: ${PROXY_PANEL_URL}
      glance.description: Proxy & port forwarding
      glance.hide: false
    environment:
      TZ: ${PROXY_TZ}
    volumes:
      - ./proxy/data:/data
      - ./proxy/letsencrypt:/etc/letsencrypt
    network_mode: service:tailscale
    healthcheck:
      test: ["CMD", "/usr/bin/check-health"]
      interval: 10s
      timeout: 3s
    depends_on:
      tailscale:
        condition: service_healthy

  socket-proxy:
    # This image is used to expose the docker socket as read-only
    image: "11notes/socket-proxy:2.1.6"
    restart: "always"
    read_only: true
    user: "0:988"
    volumes:
      - "/run/docker.sock:/run/docker.sock:ro"
      - "socket-proxy.run:/run/proxy"

  dns:
    # Ports used: 53, 3000
    image: adguard/adguardhome
    restart: unless-stopped
    labels:
      glance.name: DNS
      glance.icon: sh:adguard-home-light
      glance.url: ${DNS_PANEL_URL}
      glance.description: Plain DNS
      glance.hide: false
    volumes:
      - ./dns/work:/opt/adguardhome/work
      - ./dns/conf:/opt/adguardhome/conf
    # DNS server will not work until the initial setup is finished
    healthcheck:
      test: ["CMD-SHELL", "nc -z 127.0.0.1 53 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    depends_on:
      # The UI is exposed through the proxy service, but since the
      # proxy service also depends on this service hostname to be
      # resolvable, I cannot add a circular dependency.
      tailscale:
        condition: service_healthy

  dashboard:
    # Ports used: 8080
    image: glanceapp/glance
    restart: unless-stopped
    # read_only: true
    labels:
      glance.name: Dashboard
      glance.icon: sh:glance-light
      glance.url: ${DASHBOARD_PANEL_URL}
      glance.description: Home & status
      glance.hide: false
    environment:
      DNS_PANEL_URL: ${DNS_PANEL_INTERNAL_URL}
      DNS_PANEL_USERNAME: ${DNS_PANEL_USERNAME}
      DNS_PANEL_PASSWORD: ${DNS_PANEL_PASSWORD}
      TS_APIKEY: ${TS_APIKEY}
      WUD_ODIN_URL: ${WUD_ODIN_URL}
    volumes:
      #- /var/run/docker.sock:/var/run/docker.sock
      - "socket-proxy.run:/var/run"
      - ./dashboard/config:/app/config
    depends_on:
      proxy:
        condition: service_healthy

  bookmarks:
    # Ports used: 8080
    image: ghcr.io/go-shiori/shiori
    restart: unless-stopped
    labels:
      glance.name: Bookmarks
      glance.icon: mdi:bookmark-box
      glance.url: ${BOOKMARKS_PANEL_URL}
      glance.description: Save links
      glance.hide: false
    environment:
      SHIORI_DIR: /srv/shiori
      SHIORI_HTTP_SECRET_KEY: ${BOOKMARKS_HTTP_SECRET_KEY}
    volumes:
      - ./bookmarks/shiori:/srv/shiori
    depends_on:
      proxy:
        condition: service_healthy

  updates:
    # Ports used: 3000
    image: getwud/wud
    read_only: true
    environment:
      #WUD_AUTH_BASIC_MY_USER: ${WUD_AUTH_BASIC_MY_USER}
      #WUD_AUTH_BASIC_MY_HASH: ${WUD_AUTH_BASIC_MY_HASH}
      WUD_TRIGGER_NTFY_PUBLIC_URL: ${WUD_TRIGGER_NTFY_PUBLIC_URL}
      WUD_TRIGGER_NTFY_PUBLIC_TOPIC: ${WUD_TRIGGER_NTFY_PUBLIC_TOPIC}
      WUD_TRIGGER_NTFY_PUBLIC_SIMPLETITLE: New $${container.updateKind.kind} found for container $${container.name}
      # I feel more comfortable updating manually :)
      # WUD_TRIGGER_DOCKER_LOCAL_PRUNE: true
      
      # Odin watcher
      WUD_WATCHER_ODIN_HOST: socket-proxy
    #volumes:
      #- /var/run/docker.sock:/var/run/docker.sock  # insecure
      # - "socket-proxy.run:/var/run"               # secure (local)

volumes:
  socket-proxy.run:
